{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e440e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install \"transformers>=4.43\" peft accelerate datasets pandas scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fee8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "SFT train/eval: 795/200\n",
      "Triplet train/eval: 395/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.79s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 17,432,576 || all params: 1,738,007,552 || trainable%: 1.0030\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_3972\\34731932.py:272: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `MultiLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 400\u001b[39m\n\u001b[32m    396\u001b[39m     args.eval_steps = EVAL_STEPS\n\u001b[32m    397\u001b[39m     args.lr_scheduler_type = \u001b[33m\"\u001b[39m\u001b[33mcosine\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m trainer = \u001b[43mMultiLossTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[43m=\u001b[49m\u001b[43msft_collator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtriplet_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrip_train_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtriplet_collator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrip_collator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlambda_triplet\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLAMBDA_TRIPLET\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtriplet_margin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTRIPLET_MARGIN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlambda_infonce\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLAMBDA_INFO_NCE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtau\u001b[49m\u001b[43m=\u001b[49m\u001b[43mINFO_NCE_TAU\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    415\u001b[39m trainer.train()\n\u001b[32m    416\u001b[39m trainer.save_model(OUT_DIR)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 272\u001b[39m, in \u001b[36mMultiLossTrainer.__init__\u001b[39m\u001b[34m(self, triplet_dataset, triplet_collator, lambda_triplet, triplet_margin, lambda_infonce, tau, *args, **kwargs)\u001b[39m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args,\n\u001b[32m    265\u001b[39m              triplet_dataset: Dataset = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    266\u001b[39m              triplet_collator: TripletCollator = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    270\u001b[39m              tau=INFO_NCE_TAU,\n\u001b[32m    271\u001b[39m              **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m272\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    273\u001b[39m     \u001b[38;5;28mself\u001b[39m.triplet_dataset = triplet_dataset\n\u001b[32m    274\u001b[39m     \u001b[38;5;28mself\u001b[39m.triplet_collator = triplet_collator\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:620\u001b[39m, in \u001b[36mTrainer.__init__\u001b[39m\u001b[34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, model_init, compute_loss_func, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;66;03m# Bnb Quantized models doesn't support `.to` operation.\u001b[39;00m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    617\u001b[39m     \u001b[38;5;28mself\u001b[39m.place_model_on_device\n\u001b[32m    618\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(model, \u001b[33m\"\u001b[39m\u001b[33mquantization_method\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) != QuantizationMethod.BITS_AND_BYTES\n\u001b[32m    619\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_move_model_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;66;03m# Force n_gpu to 1 to avoid DataParallel as MP will manage the GPUs\u001b[39;00m\n\u001b[32m    623\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_model_parallel:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:913\u001b[39m, in \u001b[36mTrainer._move_model_to_device\u001b[39m\u001b[34m(self, model, device)\u001b[39m\n\u001b[32m    912\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_move_model_to_device\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, device):\n\u001b[32m--> \u001b[39m\u001b[32m913\u001b[39m     model = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    914\u001b[39m     \u001b[38;5;66;03m# Moving a model to an XLA device disconnects the tied weights, so we have to retie them.\u001b[39;00m\n\u001b[32m    915\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.parallel_mode == ParallelMode.TPU \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[33m\"\u001b[39m\u001b[33mtie_weights\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1355\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1352\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1353\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1355\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:915\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    914\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m915\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    918\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    919\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    920\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    925\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    926\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:915\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    914\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m915\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    918\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    919\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    920\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    925\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    926\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[31m[... skipping similar frames: Module._apply at line 915 (5 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:915\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    914\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m915\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    918\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    919\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    920\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    925\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    926\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:942\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    938\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    939\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    940\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    943\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    945\u001b[39m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1348\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1346\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1347\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1348\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[32m   1349\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1350\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mwhen moving module from meta to a different device.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1351\u001b[39m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1352\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1353\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mNotImplementedError\u001b[39m: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device."
     ]
    }
   ],
   "source": [
    "# train_qwen3_lora_sft_triplet_infonce.py\n",
    "import os, random, math, json, re\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Any\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM, Trainer\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch.nn.functional as F\n",
    "from packaging.version import parse as V\n",
    "import transformers\n",
    "\n",
    "# -----------------------\n",
    "# Config\n",
    "# -----------------------\n",
    "SEED = 42\n",
    "CSV_PATH = \"training_dataset/training_dataset.csv\"  \n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen3-1.7B\"\n",
    "OUT_DIR = \"aiquest_custom_qwen_1.7B\"\n",
    "\n",
    "# LoRA (no QLoRA)\n",
    "LORA_R = 16\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.05\n",
    "LORA_TARGETS = [\n",
    "    \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",          # attention projections\n",
    "    \"gate_proj\",\"up_proj\",\"down_proj\"             # MLP (FFN) projections\n",
    "]\n",
    "\n",
    "# Loss mixing\n",
    "TRIPLET_MARGIN = 0.20            \n",
    "LAMBDA_TRIPLET = 0.20           \n",
    "LAMBDA_INFO_NCE = 0.25           \n",
    "INFO_NCE_TAU = 0.07\n",
    "INFO_NCE_QUEUE = 4096            # size of memory bank\n",
    "\n",
    "# Training args\n",
    "EPOCHS = 25\n",
    "BATCH_SIZE = 1\n",
    "GRAD_ACCUM = 32\n",
    "LR = 2e-4\n",
    "EVAL_STEPS = 200\n",
    "SAVE_STEPS = 200\n",
    "MAX_LEN = 1024\n",
    "\n",
    "# -----------------------\n",
    "# Repro\n",
    "# -----------------------\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# -----------------------\n",
    "# Load & normalize data\n",
    "# -----------------------\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# normalize columns\n",
    "col_map = {\n",
    "    \"Attribute\": \"attribute\",\n",
    "    \"Anchor Element\": \"anchor\",\n",
    "    \"GeneratedClause\": \"clause\",\n",
    "    \"Candidate Clause\": \"clause\",\n",
    "    \"Label\": \"label\",\n",
    "}\n",
    "for k,v in col_map.items():\n",
    "    if k in df.columns:\n",
    "        df.rename(columns={k:v}, inplace=True)\n",
    "\n",
    "if not {\"attribute\",\"anchor\",\"clause\",\"label\"}.issubset(df.columns):\n",
    "    raise ValueError(f\"CSV must have columns: {list(col_map.keys())}\")\n",
    "\n",
    "# normalize labels\n",
    "df[\"label\"] = df[\"label\"].astype(str).str.strip().str.lower().map({\n",
    "    \"standard\":\"Standard\",\n",
    "    \"non-standard\":\"Non-Standard\",\n",
    "    \"nonstandard\":\"Non-Standard\",\n",
    "    \"non standard\":\"Non-Standard\",\n",
    "})\n",
    "df = df.dropna(subset=[\"attribute\",\"anchor\",\"clause\",\"label\"]).reset_index(drop=True)\n",
    "\n",
    "# -----------------------\n",
    "# Build triplets per anchor\n",
    "# -----------------------\n",
    "triplets = []\n",
    "for anchor, g in df.groupby(\"anchor\"):\n",
    "    pos = g[g[\"label\"]==\"Standard\"][\"clause\"].tolist()\n",
    "    neg = g[g[\"label\"]==\"Non-Standard\"][\"clause\"].tolist()\n",
    "    if not pos or not neg:\n",
    "        continue\n",
    "    m = min(len(pos), len(neg))\n",
    "    for i in range(m):\n",
    "        triplets.append({\n",
    "            \"anchor\": anchor,\n",
    "            \"pos\": pos[i % len(pos)],\n",
    "            \"neg\": neg[i % len(neg)],\n",
    "            \"attribute\": g[\"attribute\"].iloc[0]\n",
    "        })\n",
    "trip_df = pd.DataFrame(triplets)\n",
    "\n",
    "# Split by anchor to avoid leakage\n",
    "anchors_all = df[\"anchor\"].drop_duplicates().tolist()\n",
    "train_a, eval_a = train_test_split(anchors_all, test_size=0.2, random_state=SEED)\n",
    "\n",
    "sft_train = df[df[\"anchor\"].isin(train_a)].reset_index(drop=True)\n",
    "sft_eval  = df[df[\"anchor\"].isin(eval_a)].reset_index(drop=True)\n",
    "trip_train = trip_df[trip_df[\"anchor\"].isin(train_a)].reset_index(drop=True)\n",
    "trip_eval  = trip_df[trip_df[\"anchor\"].isin(eval_a)].reset_index(drop=True)\n",
    "\n",
    "print(f\"SFT train/eval: {len(sft_train)}/{len(sft_eval)}\")\n",
    "print(f\"Triplet train/eval: {len(trip_train)}/{len(trip_eval)}\")\n",
    "\n",
    "# -----------------------\n",
    "# Tokenizer & model (LoRA only; no QLoRA)\n",
    "# -----------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Keep the label tokens consistent (leading space matters for BPE)\n",
    "LABEL_TEXT = {\"Standard\": \" Standard\", \"Non-Standard\": \" Non-Standard\"}\n",
    "tokenizer.truncation_side = \"left\"   # keep label at the end if truncation happens\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float16\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    dtype=dtype,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.output_hidden_states = True\n",
    "\n",
    "lora_cfg = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    target_modules=LORA_TARGETS,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, lora_cfg)\n",
    "print(model.print_trainable_parameters())\n",
    "\n",
    "# -----------------------\n",
    "# Prompt formatting\n",
    "# -----------------------\n",
    "SYSTEM_RULES = (\n",
    "\"Decide if the candidate clause matches the standard template.\\n\"\n",
    "\"Standard: exact structural match, value substitution with same intent, or minor wording changes.\\n\"\n",
    "\"Non-Standard: carve-outs or exceptions, added conditions/timing, or different reimbursement methodologies.\"\n",
    ")\n",
    "\n",
    "def make_chat(attribute, anchor, clause):\n",
    "    user = (\n",
    "        f\"Attribute: {attribute}\\n\"\n",
    "        f\"Anchor (Standard): {anchor}\\n\"\n",
    "        f\"Candidate: {clause}\\n\"\n",
    "        f\"Answer with exactly one word: Standard or Non-Standard.\\n\"\n",
    "        f\"Label:\"\n",
    "    )\n",
    "    msgs = [\n",
    "        {\"role\":\"system\",\"content\":SYSTEM_RULES},\n",
    "        {\"role\":\"user\",\"content\":user}\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(\n",
    "        msgs, add_generation_prompt=True, tokenize=False\n",
    "    )\n",
    "\n",
    "def build_sft_dataset(frame: pd.DataFrame) -> Dataset:\n",
    "    prompts, labels = [], []\n",
    "    for r in frame.itertuples(index=False):\n",
    "        prompts.append(make_chat(r.attribute, r.anchor, r.clause))\n",
    "        labels.append(r.label)\n",
    "    return Dataset.from_dict({\"prompt\":prompts, \"label\":labels})\n",
    "\n",
    "sft_train_ds = build_sft_dataset(sft_train)\n",
    "sft_eval_ds  = build_sft_dataset(sft_eval)\n",
    "\n",
    "trip_train_ds = Dataset.from_pandas(trip_train)\n",
    "trip_eval_ds  = Dataset.from_pandas(trip_eval)\n",
    "\n",
    "# -----------------------\n",
    "# Collators\n",
    "# -----------------------\n",
    "@dataclass\n",
    "class SFTCollator:\n",
    "    max_length: int = MAX_LEN\n",
    "    def __call__(self, batch):\n",
    "        prompts = [b[\"prompt\"] for b in batch]\n",
    "        labels  = [b[\"label\"]  for b in batch]\n",
    "        # Build \"prompt + gold label\" and then mask prompt tokens\n",
    "        suffixed = [p + LABEL_TEXT[l] for p,l in zip(prompts, labels)]\n",
    "        enc = tokenizer(suffixed, padding=True, truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n",
    "        labs = enc[\"input_ids\"].clone()\n",
    "        for i, (p, l) in enumerate(zip(prompts, labels)):\n",
    "            plen = len(tokenizer(p, add_special_tokens=False)[\"input_ids\"])\n",
    "            # mask everything before the appended label\n",
    "            labs[i, :plen] = -100\n",
    "        return {\"input_ids\": enc[\"input_ids\"], \"attention_mask\": enc[\"attention_mask\"], \"labels\": labs}\n",
    "\n",
    "sft_collator = SFTCollator()\n",
    "\n",
    "@dataclass\n",
    "class TripletCollator:\n",
    "    max_length: int = MAX_LEN\n",
    "    def __call__(self, batch):\n",
    "        a = tokenizer([b[\"anchor\"] for b in batch], padding=True, truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n",
    "        p = tokenizer([b[\"pos\"] for b in batch],    padding=True, truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n",
    "        n = tokenizer([b[\"neg\"] for b in batch],    padding=True, truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n",
    "        return {\"anc\":a, \"pos\":p, \"neg\":n}\n",
    "\n",
    "trip_collator = TripletCollator()\n",
    "\n",
    "# -----------------------\n",
    "# InfoNCE feature queue\n",
    "# -----------------------\n",
    "class FeatureQueue:\n",
    "    def __init__(self, dim, max_size=INFO_NCE_QUEUE, device=None):\n",
    "        self.max_size = max_size\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.bank = torch.empty(0, dim, device=self.device)\n",
    "    @torch.no_grad()\n",
    "    def add(self, x):  # x: [B, D]\n",
    "        x = x.detach()\n",
    "        if self.bank.numel() == 0:\n",
    "            self.bank = x[-self.max_size:]\n",
    "        else:\n",
    "            self.bank = torch.cat([self.bank, x], dim=0)[-self.max_size:]\n",
    "    def get(self):\n",
    "        return self.bank if self.bank.numel() else None\n",
    "\n",
    "def mean_pool(last_hidden, attn_mask):\n",
    "    mask = attn_mask.unsqueeze(-1)  # [B,T,1]\n",
    "    summed = (last_hidden * mask).sum(dim=1)\n",
    "    denom = mask.sum(dim=1).clamp(min=1)\n",
    "    return summed / denom\n",
    "\n",
    "def info_nce_from_pairs(emb_q, emb_pos, bank, tau=INFO_NCE_TAU):\n",
    "    q = F.normalize(emb_q, dim=-1)\n",
    "    p = F.normalize(emb_pos, dim=-1)\n",
    "    pos_logit = (q * p).sum(dim=-1, keepdim=True)     # [B,1]\n",
    "    if bank is not None:\n",
    "        n = F.normalize(bank, dim=-1)                 # [M,D]\n",
    "        neg_logits = q @ n.T                          # [B,M]\n",
    "        logits = torch.cat([pos_logit, neg_logits], dim=1)\n",
    "    else:\n",
    "        logits = pos_logit\n",
    "    logits = logits / tau\n",
    "    labels = torch.zeros(q.size(0), dtype=torch.long, device=q.device)  # positive at index 0\n",
    "    return F.cross_entropy(logits, labels)\n",
    "\n",
    "# -----------------------\n",
    "# Custom Trainer (SFT + Triplet + InfoNCE)\n",
    "# -----------------------\n",
    "class MultiLossTrainer(Trainer):\n",
    "    def __init__(self, *args,\n",
    "                 triplet_dataset: Dataset = None,\n",
    "                 triplet_collator: TripletCollator = None,\n",
    "                 lambda_triplet=LAMBDA_TRIPLET,\n",
    "                 triplet_margin=TRIPLET_MARGIN,\n",
    "                 lambda_infonce=LAMBDA_INFO_NCE,\n",
    "                 tau=INFO_NCE_TAU,\n",
    "                 **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.triplet_dataset = triplet_dataset\n",
    "        self.triplet_collator = triplet_collator\n",
    "        self.lambda_triplet = lambda_triplet\n",
    "        self.triplet_margin = triplet_margin\n",
    "        self.lambda_infonce = lambda_infonce\n",
    "        self.tau = tau\n",
    "        self._trip_idx = 0\n",
    "        self._feature_queue = None  # init lazily on first pass\n",
    "\n",
    "    def get_triplet_batch(self, bsz):\n",
    "        # simple cycling batcher\n",
    "        N = len(self.triplet_dataset)\n",
    "        if N == 0:\n",
    "            return None\n",
    "        if self._trip_idx + bsz > N:\n",
    "            self._trip_idx = 0\n",
    "        sl = self.triplet_dataset[self._trip_idx:self._trip_idx + bsz]\n",
    "        self._trip_idx += bsz\n",
    "        return self.triplet_collator(sl)\n",
    "\n",
    "    def forward_hidden(self, model, batch):\n",
    "        out = model(\n",
    "            input_ids=batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        hid = out.hidden_states[-1]\n",
    "        emb = mean_pool(hid, batch[\"attention_mask\"])\n",
    "        return emb\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # 1) SFT CE loss\n",
    "        outputs = model(**inputs)\n",
    "        loss_sft = outputs.loss\n",
    "\n",
    "        # 2) Triplet + 3) InfoNCE (using the same triplet mini-batch)\n",
    "        trip_bsz = self.args.per_device_train_batch_size\n",
    "        trip = self.get_triplet_batch(trip_bsz)\n",
    "        if trip is None:\n",
    "            loss = loss_sft\n",
    "            return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "        # move to device\n",
    "        device = model.device\n",
    "        for k in [\"anc\",\"pos\",\"neg\"]:\n",
    "            for kk in trip[k]:\n",
    "                trip[k][kk] = trip[k][kk].to(device)\n",
    "\n",
    "        # forward to get pooled embeddings\n",
    "        out_anc = model(input_ids=trip[\"anc\"][\"input_ids\"], attention_mask=trip[\"anc\"][\"attention_mask\"], output_hidden_states=True)\n",
    "        out_pos = model(input_ids=trip[\"pos\"][\"input_ids\"], attention_mask=trip[\"pos\"][\"attention_mask\"], output_hidden_states=True)\n",
    "        out_neg = model(input_ids=trip[\"neg\"][\"input_ids\"], attention_mask=trip[\"neg\"][\"attention_mask\"], output_hidden_states=True)\n",
    "\n",
    "        emb_a = mean_pool(out_anc.hidden_states[-1], trip[\"anc\"][\"attention_mask\"])\n",
    "        emb_p = mean_pool(out_pos.hidden_states[-1], trip[\"pos\"][\"attention_mask\"])\n",
    "        emb_n = mean_pool(out_neg.hidden_states[-1], trip[\"neg\"][\"attention_mask\"])\n",
    "\n",
    "        # Triplet margin loss: max(0, m - cos(a,p) + cos(a,n))\n",
    "        sim_ap = F.cosine_similarity(emb_a, emb_p)\n",
    "        sim_an = F.cosine_similarity(emb_a, emb_n)\n",
    "        zero = torch.zeros_like(sim_ap)\n",
    "        loss_trip = torch.maximum(zero, self.triplet_margin - sim_ap + sim_an).mean()\n",
    "\n",
    "        # InfoNCE with queue (anchor as query, pos as key, queue = memory of previous keys/negs)\n",
    "        if self._feature_queue is None:\n",
    "            self._feature_queue = FeatureQueue(dim=emb_a.size(-1), max_size=INFO_NCE_QUEUE, device=device)\n",
    "        bank = self._feature_queue.get()\n",
    "        loss_nce = info_nce_from_pairs(emb_a, emb_p, bank, tau=self.tau)\n",
    "\n",
    "        # update queue with current step features (pos & neg both become future negatives for other anchors)\n",
    "        with torch.no_grad():\n",
    "            self._feature_queue.add(torch.cat([emb_p, emb_n], dim=0))\n",
    "\n",
    "        # Combined loss\n",
    "        loss = loss_sft + self.lambda_triplet * loss_trip + self.lambda_infonce * loss_nce\n",
    "        self.log({\n",
    "            \"loss_sft\": loss_sft.detach().float(),\n",
    "            \"loss_triplet\": loss_trip.detach().float(),\n",
    "            \"loss_infonce\": loss_nce.detach().float()\n",
    "        })\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# -----------------------\n",
    "# HF datasets\n",
    "# -----------------------\n",
    "train_ds = sft_train_ds\n",
    "eval_ds  = sft_eval_ds\n",
    "\n",
    "# -----------------------\n",
    "# Train\n",
    "# -----------------------\n",
    "\n",
    "\n",
    "ver = V(transformers.__version__)\n",
    "\n",
    "# common kwargs that are valid across versions\n",
    "base_args = dict(\n",
    "    output_dir=OUT_DIR,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM,\n",
    "    learning_rate=LR,\n",
    "    warmup_ratio=0.05,           # scheduler/warmup set below for v5\n",
    "    logging_steps=25,\n",
    "    save_steps=SAVE_STEPS,       # overridden by set_save for v5\n",
    "    bf16=(dtype==torch.bfloat16),\n",
    "    fp16=(dtype==torch.float16),\n",
    "    gradient_checkpointing=True,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "args = transformers.TrainingArguments(**base_args)\n",
    "\n",
    "if ver.major >= 5:\n",
    "    # New v5 style: use setters\n",
    "    args = args.set_evaluate(strategy=\"steps\", steps=EVAL_STEPS)\n",
    "    args = args.set_save(strategy=\"steps\", steps=SAVE_STEPS)\n",
    "    args = args.set_lr_scheduler(name=\"cosine\", warmup_ratio=0.05)\n",
    "    # (optional) logging strategy also has a setter:\n",
    "    # args = args.set_logging(strategy=\"steps\", steps=25, report_to=\"none\")\n",
    "else:\n",
    "    # Legacy v4 style fields still exist\n",
    "    args.evaluation_strategy = \"steps\"\n",
    "    args.eval_steps = EVAL_STEPS\n",
    "    args.lr_scheduler_type = \"cosine\"\n",
    "\n",
    "\n",
    "trainer = MultiLossTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    data_collator=sft_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    triplet_dataset=trip_train_ds,\n",
    "    triplet_collator=trip_collator,\n",
    "    lambda_triplet=LAMBDA_TRIPLET,\n",
    "    triplet_margin=TRIPLET_MARGIN,\n",
    "    lambda_infonce=LAMBDA_INFO_NCE,\n",
    "    tau=INFO_NCE_TAU\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(OUT_DIR)\n",
    "tokenizer.save_pretrained(OUT_DIR)\n",
    "\n",
    "print(\"Done. Model saved to:\", OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8acad3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
